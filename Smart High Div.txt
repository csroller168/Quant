"""
Copyright Chris Short 2017
Use is unauthorized without written permission

TODO
    - reduce drawdown 
        - look at daily open/current price/previous close relationships
        - factors for sorting
        - other fundamental metrics for screening
    - Test with "or" in pullout criteria
    - increase leverage
    - remove symbol filter

    X tune sma days in indicators
    X. try simple screen criteria
    X. daily drawdown check
    X. need daily rebalancing so we can screen for drawdown and still have enough stocks in bucket
    X cap % in one stock
    X re-calc market favorability daily

vary leverage on vix or market std. dev (or futures)
- test taking out blacklist filter
- research
    - build list of metrics for all Q1500 - 
        - Cache month n
        - record stats on month n+1 for top & bottom 5% ROI
        - plot min/max/avg/diff % for key metrics over time
        - revise screen criteria accordingly
        - do this programmatically month by month (possibly in next version)
- reduce drawdown
    - tune screen criteria
        - look for top offenders and screen out with fundamental metrics
        - consider adding momemtum to the screen
- tune leverage
    - vary per pullout indicators
- Check for unempl daily by watching for change in index (date posted)
- test pullout on jobs created, not unemployment rate
- refine screens
    1. Use "Factor Ranking & Analysis" to identify good long and short factors
    2. Get cross-sectional correlations of factors - see page 12 of ic research pdf and note at bottom of Factor "IC Analysis v1" notebook.  Use this to get optimal weights of factors
        - maybe do this in the alg semi-monthly automatically?
    3. SmartScore using the identified factors and optimal weights.  Do this separately for long and short buckets.
    4. Experiment to determine best leverage for long/short buckets during bear/bull markets
- consider factor IC decay - schedule times to re-assess factors (document in a notebook); 
- Do factor analysis/calculate long/short buckets separately per industry

"""
from quantopian.algorithm import attach_pipeline, pipeline_output
from quantopian.pipeline import Pipeline, CustomFactor
from quantopian.pipeline.data import morningstar
from quantopian.pipeline.data.quandl import fred_unrate
from quantopian.pipeline.filters.morningstar import Q1500US
from quantopian.pipeline.data.builtin import USEquityPricing
from scipy.stats import rankdata
import numpy as np
import pandas as pd
import scipy

# Globals
LoggingEnabled = True
UTILITIES_SECTOR = 207
REALESTATE_SECTOR = 104
NUM_LONG = 12
NUM_SHORT = 12
MAX_STOCK_PCT = 0.15

class UnemplData(object):
    def __init__(self, pastData):
        self.pastData = pastData
    
    def cache_latest(self, currUnempl):
        self.pastData.append(currUnempl)
    
    def is_favorable(self):
        current = self.pastData[-1]
        pastAvg = (self.pastData[-4] + self.pastData[-3] + self.pastData[-2]) / 3
        return current <= pastAvg

class CurrUnemplFactor(CustomFactor):
    window_length = 1
    inputs = [fred_unrate.value]
    
    def compute(self, today, assets, out, unempl):
        out[:] = unempl
        
def initialize(context):
    # Initialize context variables
    context.long_bucket = pd.Series([])
    context.short_bucket = pd.Series([])
    context.evalFavorabilityDue = True
    context.favorable = True
    context.unemplData = UnemplData(build_past_data())
    
    # Rebalance once per month
    schedule_function(set_eval_favorability_due,
                      date_rules.month_start(days_offset=3),
                      time_rules.market_close())
    
    # Record tracking variables at the end of each day.
    schedule_function(record_vars, date_rules.every_day(), time_rules.market_close())
    
    # Try to execute orders every day to cover unexecuted orders
    schedule_function(execute_orders,
                      date_rules.every_day(),
                      time_rules.market_open(minutes=5))
    
    # Set up pipeline
    pipe = Pipeline()  
    attach_pipeline(pipe, name='pipe')
    build_pipeline(pipe)
    
def build_pipeline(pipe):
    universe_filter = Q1500US()
    
    # Screen for no utilities and minimum volume
    sector_code = morningstar.asset_classification.morningstar_sector_code.latest
    util_sector_filter = (sector_code != UTILITIES_SECTOR)
    re_sector_filter = (sector_code != REALESTATE_SECTOR)
    
    symbol_filter = ~(morningstar.share_class_reference.symbol.latest.matches('TPC')) & ~(morningstar.share_class_reference.symbol.latest.matches('NFLX'))
    
    div_yld = morningstar.valuation_ratios.dividend_yield.latest
    high_div_filter = (div_yld > .03)
    no_div_filter = ~(div_yld > 0.0)
    
    # Factors
    pipe.add(CurrUnemplFactor(), 'unempl')
    pipe.add(div_yld, 'div_yld')
    ebit_mgn = morningstar.operation_ratios.ebitda_margin.latest
    pipe.add(ebit_mgn, 'ebit_mgn')
    mkt_cap = morningstar.valuation.market_cap.latest
    pipe.add(mkt_cap, 'mkt_cap')
    debtToEquity = morningstar.operation_ratios.long_term_debt_equity_ratio.latest
    pipe.add(debtToEquity, 'debtToEquity')
    roe = morningstar.operation_ratios.roe.latest
    pipe.add(roe, 'roe')
    pe = morningstar.valuation_ratios.pe_ratio.latest
    pipe.add(pe, 'pe')
    close = USEquityPricing.close.latest
    pipe.add(close, 'close')
    revenue_growth = morningstar.operation_ratios.revenue_growth.latest
    pipe.add(revenue_growth, 'revenue_growth')
    pipe.add(sector_code,'sector_code')
    symbol = morningstar.share_class_reference.symbol.latest
    pipe.add(symbol,'symbol')
    
    pipe.set_screen(universe_filter &
                    util_sector_filter &
                    re_sector_filter &
                    symbol_filter &
                    (high_div_filter | no_div_filter))
    
    return pipe

def set_eval_favorability_due(context, data):
    context.evalFavorabilityDue = True

def before_trading_start(context, data):
    pipe = pipeline_output('pipe')
    
    # Recalculate unempl data if needed
    if context.evalFavorabilityDue:
        context.unemplData.cache_latest(pipe['unempl'][0])
        context.evalFavorabilityDue = False
        
    context.favorable = momentumFavorable(data) or context.unemplData.is_favorable()
    
    # Rebalance
    context.long_bucket = pd.Series([])
    long_candidates = pipe[
        (pipe.div_yld > 0.03) &
        #(pipe.mkt_cap > 4000000000) &
        (pipe.ebit_mgn > 0.0) & 
        (data.history(pipe.index, 'price', 3, '1d').mean() > data.history(pipe.index, 'price', 7, '1d').mean())].sort_values(by='mkt_cap', ascending=False).iloc[:int(NUM_LONG*2.5)]
    context.long_bucket = []
    if len(long_candidates) > 0:
        context.long_bucket = get_bucket(long_candidates.sort_values(by='debtToEquity', ascending=True).index[:NUM_LONG], data)
    
    short_candidates = pipe[
        ~(pipe.div_yld > 0.0) &
        (pipe.mkt_cap < 15000000000) &
        (pipe.mkt_cap > 100000000) &
        (pipe.close > 15) &
        (pipe.debtToEquity > 0.5) &
        (data.current(pipe.index, 'price') < data.history(pipe.index, 'price', 5, '1d').mean()) &
        (pipe.roe < 0)].sort_values(by='pe', ascending=False).iloc[:int(NUM_SHORT*2.5)]
    context.short_bucket = []
    if len(short_candidates) > 0:
        context.short_bucket = get_bucket(short_candidates.sort_values(by='revenue_growth', ascending=True).index[:NUM_SHORT], data)

def get_bucket(symbols, data):
    prices = data.history(symbols, 'price', 300, '1d')
    daily_R = prices.pct_change().dropna()
    weights = get_reduced_correlation_weights(daily_R[symbols])
    return weights

def execute_orders(context,data):
    # Cover stocks no longer in lists
    for stock in context.portfolio.positions.iterkeys():  
        if stock not in context.long_bucket and \
            stock not in context.short_bucket and \
            data.can_trade(stock):
                order_target(stock, 0)
    
    # Calculate long/short leverage
    longLeverage = 1.2
    shortLeverage = -0.8
    if not context.favorable:
        longLeverage = 0.3
        shortLeverage = -0.3
    
    # Buy longs
    if len(context.long_bucket) > 0:
        for stock, weight in context.long_bucket.iteritems():
            if data.can_trade(stock) and not np.isnan(weight):
                order_target_percent(stock, longLeverage * np.min([weight,MAX_STOCK_PCT]))
    
    # Sell shorts
    if len(context.short_bucket) > 0:
        for stock, weight in context.short_bucket.iteritems():
            if data.can_trade(stock) and not np.isnan(weight):
                order_target_percent(stock, shortLeverage * np.max([weight,MAX_STOCK_PCT]))
    
    if len(context.short_bucket) == 0:
        order_target(sid(5061), -10)

def momentumFavorable(data):
    spy = symbol('SPY')
    mavgShort = data.history(spy, 'price', 50, '1d').mean()
    mavgLong = data.history(spy, 'price', 200, '1d').mean()
    return mavgShort > mavgLong
            
def record_vars(context, data):
    if not LoggingEnabled:
        return
    
    # Plot variables at the end of each day.
    long_count = 0
    short_count = 0
    leverage = context.account.leverage
    
    log_str = "holding: "
    for position in context.portfolio.positions.itervalues():
        if position.amount > 0:
            long_count += 1
            log_str += position.sid.symbol + ", "
        if position.amount < 0:
            short_count += 1
            log_str += position.sid.symbol + "(short), "
    
    # Raise an alert if short_count or leverage out of contest range
    if short_count == 0:
        short_count = 99
    if leverage > 3:
        leverage = 99
    
    # Plot the counts
    record(num_long=long_count, short_count=short_count, leverage=leverage)
    log.info(log_str)

def get_reduced_correlation_weights(returns, risk_adjusted=True):
    """
    Implementation of minimum correlation algorithm.
    ref: http://cssanalytics.com/doc/MCA%20Paper.pdf
    
    :Params:
        :returns <Pandas DataFrame>:Timeseries of asset returns
        :risk_adjusted <boolean>: If True, asset weights are scaled
                                  by their standard deviations
    """
    correlations = returns.corr()
    adj_correlations = get_adjusted_cor_matrix(correlations)
    initial_weights = adj_correlations.T.mean()

    ranks = initial_weights.rank()
    ranks /= ranks.sum()

    weights = adj_correlations.dot(ranks)
    weights /= weights.sum()

    if risk_adjusted:
        weights = weights / returns.std()
        weights /= weights.sum()
    return weights

def get_adjusted_cor_matrix(cor):
    values = cor.values.flatten()
    mu = np.mean(values)
    sigma = np.std(values)
    distribution = scipy.stats.norm(mu, sigma)
    return 1 - cor.apply(lambda x: distribution.cdf(x))
    
def build_past_data():
    unempl_list = [
        5.8,5.9,5.9,6,6.1,6.3,6.2,6.1,6.1,6,5.8,5.7,
        5.7,5.6,5.8,5.6,5.6,5.6,5.5,5.4,5.4,5.5,5.4,5.4,
        5.3,5.4,5.2,5.2,5.1,5,5,4.9,5,5,5,4.9,
        4.7,4.8,4.7,4.7,4.6,4.6,4.7,4.7,4.5,4.4,4.5,4.4,
        4.6,4.5,4.4,4.5,4.4,4.6,4.7,4.6,4.7,4.7,4.7,5,
        5,4.9,5.1,5,5.4,5.6,5.8,6.1,6.1,6.5,6.8,7.3,
        7.8,8.3,8.7,9,9.4,9.5,9.5,9.6,9.8,10,9.9,9.9,
        9.8,9.8,9.9,9.9,9.6,9.4,9.4,9.5,9.5,9.4,9.8,9.3,
        9.1,9,9,9.1,9,9.1,9,9,9,8.8,8.6,8.5,
        8.3,8.3,8.2,8.2,8.2,8.2,8.2,8.1,7.8,7.8,7.7,7.9,
        8,7.7,7.5,7.6,7.5,7.5,7.3,7.3,7.2,7.2,6.9,6.7,
        6.6,6.7,6.7,6.2,6.3,6.1,6.2,6.2,5.9,5.7,5.8,5.6,
        5.7,5.5,5.4,5.4,5.5,5.3,5.2,5.1,5,5,5,5,
        4.9,4.9,5,5,4.7,4.9,4.9,4.9,4.9,4.8,4.6,4.7,
        4.8,4.7,4.5]
    
    # Strip future values
    now = get_datetime()
    monthOffset = ((now.year - 2003) * 12) + now.month
    unempl_list = unempl_list[:monthOffset]
    
    return unempl_list